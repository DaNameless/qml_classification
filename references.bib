@incollection{Ayodele10,
author = {Taiwo Oladipupo Ayodele},
title = {Types of Machine Learning Algorithms},
booktitle = {New Advances in Machine Learning},
publisher = {IntechOpen},
address = {Rijeka},
year = {2010},
editor = {Yagang Zhang},
chapter = {3},
doi = {10.5772/9385},
url = {https://doi.org/10.5772/9385}
}

@misc{degruyterIntroductionQuantum,
	author = {Siddhartha Bhattacharyya},
	title = {1. {I}ntroduction to quantum machine learning --- degruyter.com},
	howpublished = {\url{https://www.degruyter.com/document/doi/10.1515/9783110670707-001/html?lang=en&srsltid=AfmBOooSaEWc-Lhu5Q72P5vuHWm_Y-YgM6DHeMRsIa9RGnSxPbyiBfNP}},
	year = {},
	note = {[Accessed 09-03-2025]}
}

@book{Nielsen_Chuang_2010, place={Cambridge}, title={Quantum Computation and Quantum Information: 10th Anniversary Edition}, publisher={Cambridge University Press}, author={Nielsen, Michael A. and Chuang, Isaac L.}, year={2010}}


@inproceedings{10047618,

  author={Gupta, Vratika and Mishra, Vinay Kumar and Singhal, Priyank and Kumar, Amit},

  booktitle={2022 11th International Conference on System Modeling \& Advancement in Research Trends (SMART)}, 

  title={An Overview of Supervised Machine Learning Algorithm}, 

  year={2022},

  volume={},

  number={},

  pages={87-92},

  abstract={Machine learning is a subset of Artificial intelligence. Algorithms for machine learning automatically learn from experience and improve from it without being explicitly programmed. Machine learning defines Supervised, Unsupervised and Reinforcement Learning. Supervised algorithms are worked on under guidance but unsupervised algorithms are worked on without guidance. Machine learning provides good accuracy in both the algorithms. This paper is describing machine learning methods, different types of supervised learning algorithms, comparison of machine learning algorithms and application of machine learning algorithms.},

  keywords={Machine learning algorithms;Error analysis;Supervised learning;Reinforcement learning;Market research;Classification algorithms;Unsupervised learning;ANN;Naïve Bayes;Supervised learning;Machine learning},

  doi={10.1109/SMART55829.2022.10047618},

  ISSN={2767-7362},

  month={Dec}}

@article{Arute2019,
abstract = {The promise of quantum computers is that certain computational tasks might be executed exponentially faster on a quantum processor than on a classical processor1. A fundamental challenge is to build a high-fidelity processor capable of running quantum algorithms in an exponentially large computational space. Here we report the use of a processor with programmable superconducting qubits2–7 to create quantum states on 53 qubits, corresponding to a computational state-space of dimension 253 (about 1016). Measurements from repeated experiments sample the resulting probability distribution, which we verify using classical simulations. Our Sycamore processor takes about 200 seconds to sample one instance of a quantum circuit a million times—our benchmarks currently indicate that the equivalent task for a state-of-the-art classical supercomputer would take approximately 10,000 years. This dramatic increase in speed compared to all known classical algorithms is an experimental realization of quantum supremacy8–14 for this specific computational task, heralding a much-anticipated computing paradigm. Quantum supremacy is demonstrated using a programmable superconducting processor known as Sycamore, taking approximately 200 seconds to sample one instance of a quantum circuit a million times, which would take a state-of-the-art supercomputer around ten thousand years to compute.},
author = {Arute, Frank and Arya, Kunal and Babbush, Ryan and Bacon, Dave and Bardin, Joseph C. and Barends, Rami and Biswas, Rupak and Boixo, Sergio and Brandao, Fernando G.S.L. and Buell, David A. and Burkett, Brian and Chen, Yu and Chen, Zijun and Chiaro, Ben and Collins, Roberto and Courtney, William and Dunsworth, Andrew and Farhi, Edward and Foxen, Brooks and Fowler, Austin and Gidney, Craig and Giustina, Marissa and Graff, Rob and Guerin, Keith and Habegger, Steve and Harrigan, Matthew P. and Hartmann, Michael J. and Ho, Alan and Hoffmann, Markus and Huang, Trent and Humble, Travis S. and Isakov, Sergei V. and Jeffrey, Evan and Jiang, Zhang and Kafri, Dvir and Kechedzhi, Kostyantyn and Kelly, Julian and Klimov, Paul V. and Knysh, Sergey and Korotkov, Alexander and Kostritsa, Fedor and Landhuis, David and Lindmark, Mike and Lucero, Erik and Lyakh, Dmitry and Mandr{\`{a}}, Salvatore and McClean, Jarrod R. and McEwen, Matthew and Megrant, Anthony and Mi, Xiao and Michielsen, Kristel and Mohseni, Masoud and Mutus, Josh and Naaman, Ofer and Neeley, Matthew and Neill, Charles and Niu, Murphy Yuezhen and Ostby, Eric and Petukhov, Andre and Platt, John C. and Quintana, Chris and Rieffel, Eleanor G. and Roushan, Pedram and Rubin, Nicholas C. and Sank, Daniel and Satzinger, Kevin J. and Smelyanskiy, Vadim and Sung, Kevin J. and Trevithick, Matthew D. and Vainsencher, Amit and Villalonga, Benjamin and White, Theodore and Yao, Z. Jamie and Yeh, Ping and Zalcman, Adam and Neven, Hartmut and Martinis, John M.},
doi = {10.1038/s41586-019-1666-5},
file = {:home/rolando/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Arute et al. - 2019 - Quantum supremacy using a programmable superconducting processor.pdf:pdf},
issn = {1476-4687},
journal = {Nature 2019 574:7779},
keywords = {Quantum information,Quantum physics},
month = {oct},
number = {7779},
pages = {505--510},
pmid = {31645734},
publisher = {Nature Publishing Group},
title = {{Quantum supremacy using a programmable superconducting processor}},
url = {https://www.nature.com/articles/s41586-019-1666-5},
volume = {574},
year = {2019}
}

@inproceedings{ccalg,
author = {Tang, Ewin},
title = {A quantum-inspired classical algorithm for recommendation systems},
year = {2019},
isbn = {9781450367059},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313276.3316310},
doi = {10.1145/3313276.3316310},
abstract = {We give a classical analogue to Kerenidis and Prakash’s quantum recommendation system, previously believed to be one of the strongest candidates for provably exponential speedups in quantum machine learning. Our main result is an algorithm that, given an m \texttimes{} n matrix in a data structure supporting certain ℓ2-norm sampling operations, outputs an ℓ2-norm sample from a rank-k approximation of that matrix in time O(poly(k)log(mn)), only polynomially slower than the quantum algorithm. As a consequence, Kerenidis and Prakash’s algorithm does not in fact give an exponential speedup over classical algorithms. Further, under strong input assumptions, the classical recommendation system resulting from our algorithm produces recommendations exponentially faster than previous classical systems, which run in time linear in m and n. The main insight of this work is the use of simple routines to manipulate ℓ2-norm sampling distributions, which play the role of quantum superpositions in the classical setting. This correspondence indicates a potentially fruitful framework for formally comparing quantum machine learning algorithms to classical machine learning algorithms.},
booktitle = {Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing},
pages = {217–228},
numpages = {12},
keywords = {sampling, recommender systems, quantum machine learning, low-rank approximation, exponential speedup},
location = {Phoenix, AZ, USA},
series = {STOC 2019}
}

@article{cq,
abstract = {We show how the quantum paradigm can be used to speed up unsupervised learning algorithms. More precisely, we explain how it is possible to accelerate learning algorithms by quantizing some of their subroutines. Quantization refers to the process that partially or totally converts a classical algorithm to its quantum counterpart in order to improve performance. In particular, we give quantized versions of clustering via minimum spanning tree, divisive clustering and k-medians that are faster than their classical analogues. We also describe a distributed version of k-medians that allows the participants to save on the global communication cost of the protocol compared to the classical version. Finally, we design quantum algorithms for the construction of a neighbourhood graph, outlier detection as well as smart initialization of the cluster centres. {\textcopyright} 2012 The Author(s).},
author = {A{\"{i}}meur, Esma and Brassard, Gilles and Gambs, S{\'{e}}bastien},
doi = {10.1007/S10994-012-5316-5/TABLES/1},
file = {:home/rolando/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/A{\"{i}}meur, Brassard, Gambs - 2013 - Quantum speed-up for unsupervised learning.pdf:pdf},
issn = {08856125},
journal = {Machine Learning},
keywords = {Clustering,Grover's algorithm,Quantum information processing,Quantum learning,Unsupervised learning},
mendeley-groups = {QML review},
month = {feb},
number = {2},
pages = {261--287},
publisher = {Springer},
title = {{Quantum speed-up for unsupervised learning}},
url = {https://link.springer.com/article/10.1007/s10994-012-5316-5},
volume = {90},
year = {2013}
}

@article{charqc,
abstract = {Atomic-level qubits in silicon are attractive candidates for large-scale quantum computing; however, their quantum properties and controllability are sensitive to details such as the number of donor atoms comprising a qubit and their precise location. This work combines machine learning techniques with million-atom simulations of scanning tunnelling microscopic (STM) images of dopants to formulate a theoretical framework capable of determining the number of dopants at a particular qubit location and their positions with exact lattice site precision. A convolutional neural network (CNN) was trained on 100,000 simulated STM images, acquiring a characterisation fidelity (number and absolute donor positions) of >98% over a set of 17,600 test images including planar and blurring noise commensurate with experimental measurements. The formalism is based on a systematic symmetry analysis and feature-detection processing of the STM images to optimise the computational efficiency. The technique is demonstrated for qubits formed by single and pairs of closely spaced donor atoms, with the potential to generalise it for larger donor clusters. The method established here will enable a high-precision post-fabrication characterisation of dopant qubits in silicon, with high-throughput potentially alleviating the requirements on the level of resources required for quantum-based characterisation, which will otherwise be a challenge in the context of large qubit arrays for universal quantum computing.},
author = {Usman, Muhammad and Wong, Yi Zheng and Hill, Charles D. and Hollenberg, Lloyd C.L.},
doi = {10.1038/s41524-020-0282-0},
file = {:home/rolando/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Usman et al. - 2020 - Framework for atomic-level characterisation of quantum computer arrays by machine learning.pdf:pdf},
issn = {2057-3960},
journal = {npj Computational Materials 2020 6:1},
keywords = {Computational methods,Electronic devices,Electronic properties and materials},
mendeley-groups = {QML review},
month = {mar},
number = {1},
pages = {1--8},
publisher = {Nature Publishing Group},
title = {{Framework for atomic-level characterisation of quantum computer arrays by machine learning}},
url = {https://www.nature.com/articles/s41524-020-0282-0},
volume = {6},
year = {2020}
}
@article{controlqc,
abstract = {Emerging reinforcement learning techniques using deep neural networks have shown great promise in control optimization. They harness non-local regularities of noisy control trajectories and facilitate transfer learning between tasks. To leverage these powerful capabilities for quantum control optimization, we propose a new control framework to simultaneously optimize the speed and fidelity of quantum computation against both leakage and stochastic control errors. For a broad family of two-qubit unitary gates that are important for quantum simulation of many-electron systems, we improve the control robustness by adding control noise into training environments for reinforcement learning agents trained with trusted-region-policy-optimization. The agent control solutions demonstrate a two-order-of-magnitude reduction in average-gate-error over baseline stochastic-gradient-descent solutions and up to a one-order-of-magnitude reduction in gate time from optimal gate synthesis counterparts. These significant improvements in both fidelity and runtime are achieved by combining new physical understandings and state-of-the-art machine learning techniques. Our results open a venue for wider applications in quantum simulation, quantum chemistry and quantum supremacy tests using near-term quantum devices.},
author = {Niu, Murphy Yuezhen and Boixo, Sergio and Smelyanskiy, Vadim N. and Neven, Hartmut},
doi = {10.1038/s41534-019-0141-3},
file = {:home/rolando/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Niu et al. - 2019 - Universal quantum control through deep reinforcement learning.pdf:pdf},
issn = {2056-6387},
journal = {npj Quantum Information 2019 5:1},
keywords = {Quantum information,Qubits},
mendeley-groups = {QML review},
month = {apr},
number = {1},
pages = {1--8},
publisher = {Nature Publishing Group},
title = {{Universal quantum control through deep reinforcement learning}},
url = {https://www.nature.com/articles/s41534-019-0141-3},
volume = {5},
year = {2019}
}
@article{readoutqc,
abstract = {Current methods for classifying measurement trajectories in superconducting qubit systems produce fidelities systematically lower than those predicted by experimental parameters. Here, we place current classification methods within the framework of machine learning (ML) algorithms and improve on them by investigating more sophisticated ML approaches. We find that nonlinear algorithms and clustering methods produce significantly higher assignment fidelities that help close the gap to the fidelity possible under ideal noise conditions. Clustering methods group trajectories into natural subsets within the data, which allows for the diagnosis of systematic errors. We find large clusters in the data associated with T1 processes and show these are the main source of discrepancy between our experimental and ideal fidelities. These error diagnosis techniques help provide a path forward to improve qubit measurements.},
archivePrefix = {arXiv},
arxivId = {1411.4994},
author = {Magesan, Easwar and Gambetta, Jay M. and C{\'{o}}rcoles, A. D. and Chow, Jerry M.},
doi = {10.1103/PHYSREVLETT.114.200501/FIGURES/6/THUMBNAIL},
eprint = {1411.4994},
issn = {10797114},
journal = {Physical Review Letters},
mendeley-groups = {QML review},
month = {may},
number = {20},
pages = {200501},
publisher = {American Physical Society},
title = {{Machine Learning for Discriminating Quantum Measurement Trajectories and Improving Readout}},
url = {https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.114.200501},
volume = {114},
year = {2015}
}

@article{10613907,
  author={Reka, S. Sofana and Karthikeyan, H. Leela and Shakil, A. Jack and Venugopal, Prakash and Muniraj, Manigandan},
  journal={IEEE Access}, 
  title={Exploring Quantum Machine Learning for Enhanced Skin Lesion Classification: A Comparative Study of Implementation Methods}, 
  year={2024},
  volume={12},
  number={},
  pages={104568-104584},
  keywords={Skin;Diseases;Accuracy;Computational modeling;Machine learning;Lesions;Machine learning algorithms;HAM10000 dataset;Qiskit;quantum machine learning;quanvolutional neural network;quantum support vector classifier;skin lesion;PennyLane},
  doi={10.1109/ACCESS.2024.3434681}
  }


@article{Lloyd2013,
   abstract = {Machine-learning tasks frequently involve problems of manipulating and classifying large numbers of vectors in high-dimensional spaces. Classical algorithms for solving such problems typically take time polynomial in the number of vectors and the dimension of the space. Quantum computers are good at manipulating high-dimensional vectors in large tensor product spaces. This paper provides supervised and unsupervised quantum machine learning algorithms for cluster assignment and cluster finding. Quantum machine learning can take time logarithmic in both the number of vectors and their dimension, an exponential speed-up over classical algorithms. In machine learning, information processors perform tasks of sorting, assembling, assimilating , and classifying information [1-2]. In supervised learning, the machine infers a function from a set of training examples. In unsupervised learning the machine tries to find hidden structure in unlabeled data. Recent studies and applications focus in particular on the problem of large-scale machine learning [2]-big data-where the training set and/or the number of features is large. Various results on quantum machine learning investigate the use of quantum information processors to perform machine learning tasks [3-9], including pattern matching [3], Probably Approximately Correct learning [4], feedback learning for quantum measurement [5], binary classifiers [6-7], and quantum support vector machines [8].},
   author = {Seth Lloyd and Masoud Mohseni and Patrick Rebentrost and Seth Lloyd and Masoud Mohseni and Patrick Rebentrost},
   doi = {10.48550/ARXIV.1307.0411},
   journal = {arXiv},
   keywords = {Quantum Physics},
   pages = {arXiv:1307.0411},
   title = {Quantum algorithms for supervised and unsupervised machine learning},
   url = {https://ui.adsabs.harvard.edu/abs/2013arXiv1307.0411L/abstract},
   year = {2013}
}
@article{swaptest,
   abstract = {Classical fingerprinting associates with each string a shorter string (its fingerprint), such that any two distinct strings can be distinguished with small error by comparing their fingerprints alone. The fingerprints cannot be made exponentially smaller than the original strings unless the parties preparing the fingerprints have access to correlated random sources. We show that fingerprints consisting of quantum information can be made exponentially smaller than the original strings without any correlations or entanglement between the parties. This implies an exponential quantum/classical gap for the equality problem in the simultaneous message passing model of communication complexity.},
   author = {Harry Buhrman and Richard Cleve and John Watrous and Ronald de Wolf},
   doi = {10.1103/PhysRevLett.87.167902},
   issn = {00319007},
   issue = {16},
   journal = {Physical Review Letters},
   month = {9},
   pages = {167902},
   pmid = {11690244},
   publisher = {American Physical Society},
   title = {Quantum Fingerprinting},
   volume = {87},
   url = {https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.87.167902},
   year = {2001}
}
@misc{kwak2021quantumneuralnetworksconcepts,
      title={Quantum Neural Networks: Concepts, Applications, and Challenges}, 
      author={Yunseok Kwak and Won Joon Yun and Soyi Jung and Joongheon Kim},
      year={2021},
      eprint={2108.01468},
      archivePrefix={arXiv},
      primaryClass={quant-ph},
      url={https://arxiv.org/abs/2108.01468}, 
}
@misc{farhi2018classificationquantumneuralnetworks,
      title={Classification with Quantum Neural Networks on Near Term Processors}, 
      author={Edward Farhi and Hartmut Neven},
      year={2018},
      eprint={1802.06002},
      archivePrefix={arXiv},
      primaryClass={quant-ph},
      url={https://arxiv.org/abs/1802.06002}, 
}
@misc{qiskittheory,
	author = {},
	title = {{Q}uantum {N}eural {N}etworks - {Q}iskit {M}achine {L}earning 0.8.2 --- qiskit-community.github.io},
	howpublished = {\url{https://qiskit-community.github.io/qiskit-machine-learning/tutorials/01_neural_networks.html}},
	year = {},
	note = {[Accessed 26-03-2025]},
}
@misc{kwon2024featuremapquantumdata,
      title={Feature Map for Quantum Data in Classification}, 
      author={Hyeokjea Kwon and Hojun Lee and Joonwoo Bae},
      year={2024},
      eprint={2303.15665},
      archivePrefix={arXiv},
      primaryClass={quant-ph},
      url={https://arxiv.org/abs/2303.15665}, 
}
@ARTICLE{10015720,
  author={Simões, Ricardo Daniel Monteiro and Huber, Patrick and Meier, Nicola and Smailov, Nikita and Füchslin, Rudolf M. and Stockinger, Kurt},
  journal={IEEE Access}, 
  title={Experimental Evaluation of Quantum Machine Learning Algorithms}, 
  year={2023},
  volume={11},
  number={},
  pages={6197-6208},
  keywords={Quantum computing;Qubit;Neural networks;Machine learning;Computers;Performance evaluation;Quantum mechanics;Machine learning;quantum computing;experimental evaluation},
  doi={10.1109/ACCESS.2023.3236409}}
@ARTICLE{9293291,
  author={Yumin, Dong and Wu, Mingqiu and Zhang, Jinlei},
  journal={IEEE Access}, 
  title={Recognition of Pneumonia Image Based on Improved Quantum Neural Network}, 
  year={2020},
  volume={8},
  number={},
  pages={224500-224512},
  keywords={Logic gates;Quantum computing;Lung;Diseases;Biological neural networks;Image recognition;Neurons;Quantum gate;pneumonia image recognition;quantum neural network;quantum particle swarm},
  doi={10.1109/ACCESS.2020.3044697}}
@INPROCEEDINGS{9528698,
  author={Kwak, Yunseok and Yun, Won Joon and Jung, Soyi and Kim, Joongheon},
  booktitle={2021 Twelfth International Conference on Ubiquitous and Future Networks (ICUFN)}, 
  title={Quantum Neural Networks: Concepts, Applications, and Challenges}, 
  year={2021},
  volume={},
  number={},
  pages={413-416},
  keywords={Deep learning;Training;Artificial neural networks;Quantum circuit},
  doi={10.1109/ICUFN49451.2021.9528698}}

@article{SMAILOVIC2014181,
title = {Stream-based active learning for sentiment analysis in the financial domain},
journal = {Information Sciences},
volume = {285},
pages = {181-203},
year = {2014},
note = {Processing and Mining Complex Data Streams},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2014.04.034},
url = {https://www.sciencedirect.com/science/article/pii/S0020025514004885},
author = {Jasmina Smailović and Miha Grčar and Nada Lavrač and Martin Žnidaršič},
keywords = {Predictive sentiment analysis, Stream-based active learning, Stock market, Twitter, Positive sentiment probability, Granger causality},
abstract = {Studying the relationship between public sentiment and stock prices has been the focus of several studies. This paper analyzes whether the sentiment expressed in Twitter feeds, which discuss selected companies and their products, can indicate their stock price changes. To address this problem, an active learning approach was developed and applied to sentiment analysis of tweet streams in the stock market domain. The paper first presents a static Twitter data analysis problem, explored in order to determine the best Twitter-specific text preprocessing setting for training the Support Vector Machine (SVM) sentiment classifier. In the static setting, the Granger causality test shows that sentiments in stock-related tweets can be used as indicators of stock price movements a few days in advance, where improved results were achieved by adapting the SVM classifier to categorize Twitter posts into three sentiment categories of positive, negative and neutral (instead of positive and negative only). These findings were adopted in the development of a new stream-based active learning approach to sentiment analysis, applicable in incremental learning from continuously changing financial tweet streams. To this end, a series of experiments was conducted to determine the best querying strategy for active learning of the SVM classifier adapted to sentiment analysis of financial tweet streams. The experiments in analyzing stock market sentiments of a particular company show that changes in positive sentiment probability can be used as indicators of the changes in stock closing prices.}
}

@article{gmd-16-6433-2023,
AUTHOR = {de Burgh-Day, C. O. and Leeuwenburg, T.},
TITLE = {Machine learning for numerical weather and climate modelling: a review},
JOURNAL = {Geoscientific Model Development},
VOLUME = {16},
YEAR = {2023},
NUMBER = {22},
PAGES = {6433--6477},
URL = {https://gmd.copernicus.org/articles/16/6433/2023/},
DOI = {10.5194/gmd-16-6433-2023}
}
@article{doi:10.1148/rg.2017160130,
author = {Erickson, Bradley J. and Korfiatis, Panagiotis and Akkus, Zeynettin and Kline, Timothy L.},
title = {Machine Learning for Medical Imaging},
journal = {RadioGraphics},
volume = {37},
number = {2},
pages = {505-515},
year = {2017},
doi = {10.1148/rg.2017160130},
    note ={PMID: 28212054},

URL = { 
    
        https://doi.org/10.1148/rg.2017160130
    
    

},
eprint = { 
    
        https://doi.org/10.1148/rg.2017160130
    
    

}
,
    abstract = { Machine learning is a technique for recognizing patterns that can be applied to medical images. Although it is a powerful tool that can help in rendering medical diagnoses, it can be misapplied. Machine learning typically begins with the machine learning algorithm system computing the image features that are believed to be of importance in making the prediction or diagnosis of interest. The machine learning algorithm system then identifies the best combination of these image features for classifying the image or computing some metric for the given image region. There are several methods that can be used, each with different strengths and weaknesses. There are open-source versions of most of these machine learning methods that make them easy to try and apply to images. Several metrics for measuring the performance of an algorithm exist; however, one must be aware of the possible associated pitfalls that can result in misleading metrics. More recently, deep learning has started to be used; this method has the benefit that it does not require image feature identification and calculation as a first step; rather, features are identified as part of the learning process. Machine learning has been used in medical imaging and will have a greater influence in the future. Those working in medical imaging must be aware of how machine learning works.©RSNA, 2017 }
}

@article{LIU2017159,
title = {Materials discovery and design using machine learning},
journal = {Journal of Materiomics},
volume = {3},
number = {3},
pages = {159-177},
year = {2017},
note = {High-throughput Experimental and Modeling Research toward Advanced Batteries},
issn = {2352-8478},
doi = {https://doi.org/10.1016/j.jmat.2017.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S2352847817300515},
author = {Yue Liu and Tianlu Zhao and Wangwei Ju and Siqi Shi},
keywords = {New materials discovery, Materials design, Materials properties prediction, Machine learning},
abstract = {The screening of novel materials with good performance and the modelling of quantitative structure-activity relationships (QSARs), among other issues, are hot topics in the field of materials science. Traditional experiments and computational modelling often consume tremendous time and resources and are limited by their experimental conditions and theoretical foundations. Thus, it is imperative to develop a new method of accelerating the discovery and design process for novel materials. Recently, materials discovery and design using machine learning have been receiving increasing attention and have achieved great improvements in both time efficiency and prediction accuracy. In this review, we first outline the typical mode of and basic procedures for applying machine learning in materials science, and we classify and compare the main algorithms. Then, the current research status is reviewed with regard to applications of machine learning in material property prediction, in new materials discovery and for other purposes. Finally, we discuss problems related to machine learning in materials science, propose possible solutions, and forecast potential directions of future research. By directly combining computational studies with experiments, we hope to provide insight into the parameters that affect the properties of materials, thereby enabling more efficient and target-oriented research on materials discovery and design.}
}

@article{Franco2021,
   abstract = {Machine learning (ML), and particularly algorithms based on artificial neural networks (ANNs), constitute a field of research lying at the intersection of different disciplines such as mathematics, statistics, computer science and neuroscience. This approach is characterized by the use of algorithms to extract knowledge from large and heterogeneous data sets. In addition to offering a brief introduction to ANN algorithms-based ML, in this paper we will focus our attention on its possible applications in the social sciences and, in particular, on its potential in the data analysis procedures. In this regard, we will provide three examples of applications on sociological data to assess the impact of ML in the study of relationships between variables. Finally, we will compare the potential of ML with traditional data analysis models.},
   author = {Giovanni Di Franco and Michele Santurro},
   doi = {10.1007/s11135-020-01037-y},
   isbn = {0123456789},
   keywords = {Deep learning Artificial neural network,Linear models,Machine learning,Nonlinear models,Supervised learning},
   pages = {1007-1025},
   title = {Machine learning, artificial neural networks and social research},
   volume = {55},
   url = {https://doi.org/10.1007/s11135-020-01037-y},
   year = {2021}
}

@article{Richards2019,
   abstract = {Systems neuroscience seeks explanations for how the brain implements a wide variety of perceptual, cognitive and motor tasks. Conversely, artificial intelligence attempts to design computational systems based on the tasks they will have to solve. In artificial neural networks, the three components specified by design are the objective functions, the learning rules and the architectures. With the growing success of deep learning, which utilizes brain-inspired architectures, these three designed components have increasingly become central to how we model, engineer and optimize complex artificial learning systems. Here we argue that a greater focus on these components would also benefit systems neuroscience. We give examples of how this optimization-based framework can drive theoretical and experimental progress in neuroscience. We contend that this principled perspective on systems neuroscience will help to generate more rapid progress. A deep network is best understood in terms of components used to design it—objective functions, architecture and learning rules—rather than unit-by-unit computation. Richards et al. argue that this inspires fruitful approaches to systems neuroscience.},
   author = {Blake A. Richards and Timothy P. Lillicrap and Philippe Beaudoin and Yoshua Bengio and Rafal Bogacz and Amelia Christensen and Claudia Clopath and Rui Ponte Costa and Archy de Berker and Surya Ganguli and Colleen J. Gillon and Danijar Hafner and Adam Kepecs and Nikolaus Kriegeskorte and Peter Latham and Grace W. Lindsay and Kenneth D. Miller and Richard Naud and Christopher C. Pack and Panayiota Poirazi and Pieter Roelfsema and João Sacramento and Andrew Saxe and Benjamin Scellier and Anna C. Schapiro and Walter Senn and Greg Wayne and Daniel Yamins and Friedemann Zenke and Joel Zylberberg and Denis Therien and Konrad P. Kording},
   doi = {10.1038/s41593-019-0520-2},
   issn = {1546-1726},
   issue = {11},
   journal = {Nature Neuroscience 2019 22:11},
   keywords = {Learning algorithms,Machine learning,Neural circuits},
   month = {10},
   pages = {1761-1770},
   pmid = {31659335},
   publisher = {Nature Publishing Group},
   title = {A deep learning framework for neuroscience},
   volume = {22},
   url = {https://www.nature.com/articles/s41593-019-0520-2},
   year = {2019}
}
